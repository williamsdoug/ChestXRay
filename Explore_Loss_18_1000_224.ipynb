{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Loss - 224x224\n",
    "\n",
    "### Findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.metrics import error_rate\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import gmean, hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai_addons   #add plot2 extension -- learn.recorder.plot2()\n",
    "from fastai_addons import interpretation_summary, plot_confusion_matrix, \\\n",
    "                          get_accuracy, analyze_confidence, accuracy_vs_threshold, \\\n",
    "                          show_incremental_accuracy,  analyze_low_confidence, \\\n",
    "                          plot_confusion_matrix_thresh, get_val_stats, model_cutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pneumonia_loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pneumonia_loaders import show_categories\n",
    "from pneumonia_new_loader import get_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_best_stats, show_results, show_results\n",
    "from helpers import StatsRepo, stats_repo_unit_test\n",
    "from helpers import _get_learner, _get_interp, analyze_interp, _do_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18\n",
    "size=224\n",
    "n_samples=1000    # total number of training samples\n",
    "variant='loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bs = {225:31, 448:31, 896:7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = Path()/'data'\n",
    "model_base = Path('D:/Users/Doug/Models/chest_xray')\n",
    "\n",
    "all_bs = {224:31, 448:31, 896:7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'18_{size}{variant}'\n",
    "short_prefix = f'18_{size}{variant}'\n",
    "\n",
    "data_subdir  = f'chest_xray_{size}'\n",
    "model_subdir = f'18_{size}'\n",
    "\n",
    "if size in [224, 448, 896]:\n",
    "    path = data_base/data_subdir\n",
    "    model_dir = (model_base/model_subdir).absolute()\n",
    "    bs = all_bs[size]\n",
    "else:\n",
    "    raise Exception('Unknown size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transforms():\n",
    "    tfms = get_transforms(do_flip=True, flip_vert=False,\n",
    "                          max_zoom=1.3, max_lighting=0.3)\n",
    "    return tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(all_lr=[1e-2, 3e-3, 1e-3], all_cycles=[1,2,4, 8]):\n",
    "    global short_prefix\n",
    "    for lr in all_lr:\n",
    "        for cycles in all_cycles:\n",
    "            key = f'{short_prefix}_lr_{lr}_cyc_{cycles}'\n",
    "            yield key, {'cycles':cycles, 'max_lr':lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_learner(name, ps=0.5, compute_baseline=True):\n",
    "    learn = get_learner(ps=ps)\n",
    "    learn.load(name)\n",
    "    \n",
    "    if compute_baseline:\n",
    "        base = learn.svalidate()\n",
    "        base = {'loss': base[0], 'error_rate': float(base[1]), 'accuracy': float(base[2])}\n",
    "    else:\n",
    "        base = None\n",
    "    return learn, base\n",
    "\n",
    "def refine_learner(name, ps=0.8, lr=1e-6, cycles=16, **kwargs):\n",
    "    learn, base = restore_learner(name, ps=ps)\n",
    "    print('base:', base)\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(cycles, max_lr=slice(lr,lr),\n",
    "                        callbacks=[SaveModelCallback(learn, every='improvement', \n",
    "                                   monitor='accuracy', name='best')], **kwargs)\n",
    "    stats = get_best_stats(learn)\n",
    "    \n",
    "    if stats['error_rate'] < base['error_rate']:\n",
    "        new_name = f'{name}_r_lr{lr}_c{cycles}_ps{ps}'\n",
    "        print('keeping:', stats)\n",
    "        return learn, stats, new_name\n",
    "    else:\n",
    "        print('ignoring:', stats)\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize stats repo (use prior contents if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = StatsRepo(prefix, force_init=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Training/Validation/Test dataset for  Normal / Viral / Bacterial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = get_db_np(path, size=size, scale=10, tfms=my_transforms())\n",
    "data, data_test = get_db(path, kind='nvb', n_samples=n_samples, scale=1, include_test=True,\n",
    "                  size=size, bs=bs, tfms=my_transforms())\n",
    "print('Training set:')\n",
    "show_categories(data.train_ds.y)\n",
    "print('\\nValidation set:')\n",
    "show_categories(data.valid_ds.y)\n",
    "print('\\nTest set:')\n",
    "show_categories(data_test.valid_ds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define configuration specific functions and initialize stats repo (use prior contents if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_learner = partial(_get_learner, db=data, model=model, model_dir=model_dir)\n",
    "get_learner_test = partial(_get_learner, db=data_test, model=model, model_dir=model_dir)\n",
    "\n",
    "get_interp = partial(_get_interp, get_learner=get_learner)\n",
    "get_interp_test = partial(_get_interp, get_learner=get_learner_test)\n",
    "\n",
    "do_train = partial(_do_train, get_learner=get_learner, stats_repo=all_stats, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import gmean, hmean\n",
    "\n",
    "def compute_acc(preds, y_true):\n",
    "    yy = np.argmax(preds, axis=-1)\n",
    "    return np.mean(yy==y_true)\n",
    "    \n",
    "\n",
    "def combine_predictions(all_interp):\n",
    "    y_true = to_np(all_interp[0][1].y_true)\n",
    "    all_preds = np.stack([to_np(interp.preds) for _, interp in all_interp])\n",
    "    \n",
    "    preds = np.mean(all_preds, axis=0)\n",
    "    acc_m = compute_acc(preds, y_true) \n",
    "    \n",
    "    preds = np.median(all_preds, axis=0)\n",
    "    acc_med = compute_acc(preds, y_true)\n",
    "    \n",
    "    preds = gmean(all_preds, axis=0)\n",
    "    acc_g = compute_acc(preds, y_true)\n",
    "    \n",
    "    preds = hmean(all_preds, axis=0)\n",
    "    acc_h = compute_acc(preds, y_true)\n",
    "    \n",
    "    print(f'accuracy -- mean: {acc_m:0.3f}   median: {acc_med:0.3f}   gmean: {acc_g:0.3f}   hmean: {acc_h:0.3f}')\n",
    "    return acc_m, acc_med, acc_g, acc_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Development Tests\n",
    "Disabled by default"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# verify stats\n",
    "stats_repo_unit_test(prefix='unit_test')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# verify batch size -- test for no memory overflow\n",
    "learn = get_learner()\n",
    "learn.unfreeze()\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn = get_learner()\n",
    "learn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# verify version with model cutting\n",
    "learn = get_learner(cut=5)\n",
    "learn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key, params in get_params(all_cycles=[1], all_lr=[0.01, 0.001]):\n",
    "    do_train(key, **params)\n",
    "all_stats.save()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_stats.get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_results(all_stats.get(), show_details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reinitialization disabled by default\n",
    "all_stats.clear()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# no longer necessary, prior values are restored by default during all_stats onject creation\n",
    "#all_stats.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barrier after initialization for notebook restart\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LR Finder runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot2()\n",
    "learn.recorder.plot(k=5, suggestion=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn = get_learner(cut=4)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot2()\n",
    "learn.recorder.plot(k=5, suggestion=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "learn = get_learner(cut=5)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot2()\n",
    "learn.recorder.plot(k=5, suggestion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for key, params in get_params(all_cycles=[8], all_lr=[0.01]):\n",
    "        key = f'{key}_i{i}'\n",
    "        do_train(key, **params)\n",
    "all_stats.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(all_stats.get(), limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for key, params in get_params(all_cycles=[8], all_lr=[0.02]):\n",
    "        key = f'{key}_i{i}'\n",
    "        do_train(key, **params)\n",
    "all_stats.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(all_stats.get(), limit=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for i in range(10):\n",
    "    for key, params in get_params(all_cycles=[8], all_lr=[0.03]):\n",
    "        key = f'{key}_i{i}'\n",
    "        do_train(key, **params)\n",
    "all_stats.save()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_results(all_stats.get(), limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [None, 'lr_0.03', 'lr_0.02', 'lr_0.01', 'lr_0.003', 'lr_0.001', 'lr_0.0003','lr_0.0001',\n",
    "            'cyc_4', 'cyc_8', 'cyc_12',  'cyc_16', \n",
    "            '_m', 'lr_0.02_cyc_8', 'lr_0.01_cyc_8']:\n",
    "    show_results(all_stats.get(), key=key, show_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best results sorted by error rate')\n",
    "show_results(all_stats.get(), limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best results sorted by loss')\n",
    "show_results(all_stats.get(), limit=10, sort_param='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['lr_0.03', 'lr_0.02', 'lr_0.01', 'lr_0.003', 'lr_0.001', 'lr_0.0003','lr_0.0001',\n",
    "            'cyc_4', 'cyc_8', 'cyc_12', 'cyc_16', \n",
    "            '_m', 'lr_0.02_cyc_8', 'lr_0.01_cyc_8']:\n",
    "    show_results(all_stats.get(), key=key, limit=6)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- try composite of 4 learners\n",
    "- try test-time augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '18_448cut_lr_0.02_cyc_16_i6_m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = get_interp(name)\n",
    "print(f'\\n{name} tta=False')\n",
    "analyze_interp(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = get_interp(name, use_tta=True)\n",
    "print(f'\\n{name} tta=True')\n",
    "analyze_interp(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [name for name, _ in sorted(all_stats.get(), key=lambda x:x[1]['error_rate'])[:4]]\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_interp = [[name, get_interp(name)] for name in top_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, interp in top_interp:\n",
    "    print(name)\n",
    "    analyze_interp(interp, include_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_interp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combine_predictions(top_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again using loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [name for name, _ in sorted(all_stats.get(), key=lambda x:x[1]['loss'])[:4]]\n",
    "top_interp = [[name, get_interp(name)] for name in top_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combine_predictions(top_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [name for name, _ in sorted(all_stats.get(), key=lambda x:x[1]['error_rate'])[:4]]\n",
    "top_interp = [[name, get_interp_test(name)] for name in top_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, interp in top_interp:\n",
    "    print(name)\n",
    "    analyze_interp(interp, include_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combine_predictions(top_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [name for name, _ in sorted(all_stats.get(), key=lambda x:x[1]['loss'])[:4]]\n",
    "top_interp = [[name, get_interp_test(name)] for name in top_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combine_predictions(top_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with fine tuning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '18_448cut_lr_0.02_cyc_16_i6_m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot2()\n",
    "learn.recorder.plot(k=5, suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr=slice(2e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, max_lr=slice(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, max_lr=slice(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, max_lr=slice(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner()\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, max_lr=slice(2e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, max_lr=slice(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, max_lr=slice(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, max_lr=slice(2e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, max_lr=slice(2e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, max_lr=slice(1e-6, 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, max_lr=slice(1e-6, 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(ps=0.8)\n",
    "learn.load(name)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(16, max_lr=slice(1e-6, 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, stats, new_name = refine_learner(name, cycles=2)\n",
    "stats, new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, stats, new_name = refine_learner(name, cycles=8)\n",
    "stats, new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = []\n",
    "for i in range(10):\n",
    "    learn, stats, new_name = refine_learner(name, cycles=8)\n",
    "    foo.append([1, stats])\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = []\n",
    "for i in range(10):\n",
    "    learn, stats, new_name = refine_learner(name, cycles=16)\n",
    "    foo.append([1, stats])\n",
    "    print('\\n')\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
